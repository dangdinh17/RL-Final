{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import os\n",
    "\n",
    "\n",
    "class UR3SortingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        if p.isConnected():\n",
    "            p.disconnect()\n",
    "        # Action space: Mỗi khớp có thể điều khiển với một giá trị (-1, 1)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(6,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: Trạng thái robot + thông tin vật (vị trí, màu sắc)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"robot_state\": spaces.Box(low=-1, high=1, shape=(6,), dtype=np.float32),  # 6 khớp\n",
    "            \"object_info\": spaces.Box(low=0, high=1, shape=(4,), dtype=np.float32)    # x, y, z, màu sắc\n",
    "        })\n",
    "        \n",
    "        # PyBullet settings\n",
    "        self.client = p.connect(p.GUI)\n",
    "        if self.client < 0:\n",
    "            raise RuntimeError(\"Failed to connect to PyBullet physics server.\")\n",
    "        \n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        pb_data_path = pybullet_data.getDataPath()\n",
    "        project_path = os.getcwd()\n",
    "        # Load environment\n",
    "        self.robot_uid = p.loadURDF(os.path.join(project_path, \"urdf/ur5_rg2.urdf\"), useFixedBase=True)\n",
    "        self.table_uid = p.loadURDF(os.path.join(pb_data_path, \"table/table.urdf\"), basePosition=[0.5, 0, -0.65])\n",
    "        self.tray_uid = p.loadURDF(os.path.join(pb_data_path, \"tray/traybox.urdf\"), basePosition=[0.65, 0, 0])\n",
    "        self.objects = self._load_objects()\n",
    "        \n",
    "        \n",
    "        self.joint_index_last             = 13\n",
    "        self.joint_index_endeffector_base = 7\n",
    "        p.resetDebugVisualizerCamera(cameraDistance=1.5, cameraYaw=0, cameraPitch=-40, cameraTargetPosition=[0.55, -0.35, 0.2])\n",
    "\n",
    "        self.rest_pose:np.ndarray = np.array((\n",
    "            0.0,    # Base  (Fixed)\n",
    "            0.0,    # Joint 1\n",
    "            -2.094, # Joint 2\n",
    "            1.57,   # Joint 3\n",
    "            -1.047, # Joint 4\n",
    "            -1.57,  # Joint 5\n",
    "            0,      # Joint 6\n",
    "            0.0,    # EE Base (Fixed)\n",
    "            0.785,  # EE Finger\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def _load_objects(self):\n",
    "        \"\"\"Load objects with different colors.\"\"\"\n",
    "        objects = []\n",
    "        positions = []\n",
    "        for i, color in enumerate([[1, 0, 0], [0, 1, 0], [0, 0, 1]]):  # Red, Green, Blue\n",
    "            while True:\n",
    "                # Tạo ngẫu nhiên vị trí đối tượng\n",
    "                pos = [np.random.uniform(-0.6, 0.6), 0, np.random.uniform(-0.2, 0.2)]\n",
    "                # Kiểm tra xem vị trí này có chồng chéo với các vị trí khác hay không\n",
    "                if all(np.linalg.norm(np.array(pos) - np.array(existing_pos)) > 0.5 for existing_pos in positions):\n",
    "                    positions.append(pos)\n",
    "                    break\n",
    "            \n",
    "            obj_uid = p.loadURDF(\"random_urdfs/000/000.urdf\", basePosition=pos)\n",
    "            p.changeVisualShape(obj_uid, -1, rgbaColor=color + [1])  # Set object color\n",
    "            objects.append({\"uid\": obj_uid, \"color\": color})\n",
    "        return objects\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment.\"\"\"\n",
    "        # Reset robot to initial position\n",
    "        self.current_pose:np.ndarray = np.copy(self.rest_pose)\n",
    "        for i in range(6):  # 6 khớp điều khiển\n",
    "            p.resetJointState(self.robot_uid, i, self.current_pose[i])\n",
    "            \n",
    "        self._finger_control(self.current_pose[self.joint_index_endeffector_base+1])\n",
    "        # Reset objects\n",
    "        \n",
    "        for obj in self.objects:\n",
    "            pos = [0.6, np.random.uniform(-0.2, 0.2), 0.1]\n",
    "            p.resetBasePositionAndOrientation(obj[\"uid\"], pos, [0, 0, 0, 1])\n",
    "        \n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _finger_control(self, target):\n",
    "        '''\n",
    "        Control the finger joints to target position.\n",
    "        This is to imitate the mimic joint in ROS.\n",
    "        '''\n",
    "        # Just a hardcoded control...\n",
    "        p.setJointMotorControl2(self.robot_uid, self.joint_index_endeffector_base+1, p.POSITION_CONTROL, \n",
    "                                targetPosition = target)\n",
    "        p.setJointMotorControl2(self.robot_uid, self.joint_index_endeffector_base+4, p.POSITION_CONTROL, \n",
    "                                targetPosition = target)\n",
    "        # Get the current joint pos and vel\n",
    "        finger_left = p.getJointState(self.robot_uid, self.joint_index_endeffector_base+1)\n",
    "        finger_right = p.getJointState(self.robot_uid, self.joint_index_endeffector_base+4)\n",
    "        # Propagate it to the other joints.\n",
    "        p.setJointMotorControl2(self.robot_uid, self.joint_index_endeffector_base+2, p.POSITION_CONTROL, \n",
    "                                targetPosition = finger_left[0], \n",
    "                                targetVelocity = finger_left[1],\n",
    "                                positionGain=1.2)\n",
    "        p.setJointMotorControl2(self.robot_uid, self.joint_index_endeffector_base+3, p.POSITION_CONTROL, \n",
    "                                targetPosition = finger_left[0], \n",
    "                                targetVelocity = finger_left[1],\n",
    "                                positionGain=1.2)\n",
    "        p.setJointMotorControl2(self.robot_uid, self.joint_index_endeffector_base+5, p.POSITION_CONTROL, \n",
    "                                targetPosition = finger_right[0], \n",
    "                                targetVelocity = finger_right[1],\n",
    "                                positionGain=1.2)\n",
    "        p.setJointMotorControl2(self.robot_uid, self.joint_index_endeffector_base+6, p.POSITION_CONTROL, \n",
    "                                targetPosition = finger_right[0], \n",
    "                                targetVelocity = finger_right[1],\n",
    "                                positionGain=1.2)\n",
    "        \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current observation.\"\"\"\n",
    "        # Robot joint positions\n",
    "        joint_states = [p.getJointState(self.robot_uid, i)[0] for i in range(6)]\n",
    "        robot_state = np.array(joint_states) / np.pi  # Normalize to [-1, 1]\n",
    "        \n",
    "        # Object position and color\n",
    "        obj = self.objects[0]  # For simplicity, focus on the first object\n",
    "        pos, _ = p.getBasePositionAndOrientation(obj[\"uid\"])\n",
    "        color = obj[\"color\"]\n",
    "        object_info = np.array(list(pos) + [color[0]])  # Include only red channel as example\n",
    "        \n",
    "        return {\"robot_state\": robot_state, \"object_info\": object_info}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step.\"\"\"\n",
    "        # Scale action to joint range\n",
    "        joint_positions = np.clip(action, -1, 1) * np.pi  # Scale to [-π, π]\n",
    "        \n",
    "        # Apply action\n",
    "        for i in range(6):\n",
    "            p.setJointMotorControl2(self.robot_uid, i, p.POSITION_CONTROL, targetPosition=joint_positions[i])\n",
    "        p.stepSimulation()\n",
    "        \n",
    "        # Get observation\n",
    "        obs = self._get_observation()\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self._compute_reward(obs)\n",
    "        \n",
    "        # Check termination\n",
    "        done = self._check_termination(obs)\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def _compute_reward(self, obs):\n",
    "        \"\"\"Compute reward based on task progress.\"\"\"\n",
    "        # Example: Reward for proximity to the object and correct classification\n",
    "        obj_pos = obs[\"object_info\"][:3]\n",
    "        dist_to_obj = np.linalg.norm(obj_pos - np.array([0.6, 0, 0.1]))  # Desired position\n",
    "        is_correct_color = obs[\"object_info\"][3] > 0.8  # Red color\n",
    "        reward = -dist_to_obj\n",
    "        if is_correct_color:\n",
    "            reward += 1\n",
    "        return reward\n",
    "\n",
    "    def _check_termination(self, obs):\n",
    "        \"\"\"Check if episode should terminate.\"\"\"\n",
    "        obj_pos = obs[\"object_info\"][:3]\n",
    "        dist_to_obj = np.linalg.norm(obj_pos - np.array([0.6, 0, 0.1]))\n",
    "        return dist_to_obj < 0.05  # Close enough to classify\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass  # GUI rendering is handled by PyBullet\n",
    "    \n",
    "    def close(self):\n",
    "        p.disconnect(self.client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = UR3SortingEnv()\n",
    "print(\"Environment initialized successfully!\")\n",
    "# model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=10000)\n",
    "# model.save(\"ur3_sorting_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, memory_size: int) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=self.memory_size)\n",
    "\n",
    "    def add(self, experience) -> None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int, continuous: bool = True):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if continuous:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "        else:\n",
    "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "            return [self.buffer[i] for i in indexes]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        self.alpha = 4\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def getV(self, q_value):\n",
    "        v = self.alpha * torch.log(torch.sum(torch.exp(q_value/self.alpha), dim=1, keepdim=True))\n",
    "        return v\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        # print('state : ', state)\n",
    "        with torch.no_grad():\n",
    "            q = self.forward(state)\n",
    "            v = self.getV(q).squeeze()\n",
    "            # print('q & v', q, v)\n",
    "            dist = torch.exp((q-v)/self.alpha)\n",
    "            # print(dist)\n",
    "            dist = dist / torch.sum(dist)\n",
    "            # print(dist)\n",
    "            c = Categorical(dist)\n",
    "            a = c.sample()\n",
    "        return a.item()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    onlineQNetwork = SoftQNetwork().to(device)\n",
    "    targetQNetwork = SoftQNetwork().to(device)\n",
    "    targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
    "\n",
    "    optimizer = torch.optim.Adam(onlineQNetwork.parameters(), lr=1e-4)\n",
    "\n",
    "    GAMMA = 0.99\n",
    "    REPLAY_MEMORY = 50000\n",
    "    BATCH = 16\n",
    "    UPDATE_STEPS = 4\n",
    "\n",
    "    memory_replay = Memory(REPLAY_MEMORY)\n",
    "    writer = SummaryWriter('logs/sql')\n",
    "\n",
    "    learn_steps = 0\n",
    "    begin_learn = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    for epoch in count():\n",
    "        state, _ = env.reset()\n",
    "        if state is None or len(state) != 4:\n",
    "            print(\"State is invalid:\", state)\n",
    "        episode_reward = 0\n",
    "        for time_steps in range(200):\n",
    "            action = onlineQNetwork.choose_action(state)\n",
    "            # print(action)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if next_state is None or len(next_state) != 4:\n",
    "                print(\"Next state is invalid:\", next_state)\n",
    "                break\n",
    "            memory_replay.add((state, next_state, action, reward, done))\n",
    "            state = next_state\n",
    "            # try:\n",
    "            #     action = onlineQNetwork.choose_action(state)\n",
    "            #     next_state, reward, done, _, _ = env.step(action)\n",
    "            #     episode_reward += reward\n",
    "            #     if next_state is None or len(next_state) != 4:\n",
    "            #         print(\"Next state is invalid:\", next_state)\n",
    "            #         break\n",
    "            #     memory_replay.add((state, next_state, action, reward, done))\n",
    "            #     state = next_state\n",
    "            # except Exception as e:\n",
    "            #     # print(f\"Error occurred at state: {state}, action: {action}\")\n",
    "            #     break\n",
    "\n",
    "            if memory_replay.size() > 128:\n",
    "                if begin_learn is False:\n",
    "                    print('learn begin!')\n",
    "                    begin_learn = True\n",
    "                learn_steps += 1\n",
    "                if learn_steps % UPDATE_STEPS == 0:\n",
    "                    targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
    "                batch = memory_replay.sample(BATCH, False)\n",
    "                batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)\n",
    "\n",
    "                batch_state = torch.FloatTensor(batch_state).to(device)\n",
    "                batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
    "                batch_action = torch.FloatTensor(batch_action).unsqueeze(1).to(device)\n",
    "                batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
    "                batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_q = targetQNetwork(batch_next_state)\n",
    "                    next_v = targetQNetwork.getV(next_q)\n",
    "                    y = batch_reward + (1 - batch_done) * GAMMA * next_v\n",
    "\n",
    "                loss = F.mse_loss(onlineQNetwork(batch_state).gather(1, batch_action.long()), y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                writer.add_scalar('loss', loss.item(), global_step=learn_steps)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        writer.add_scalar('episode reward', episode_reward, global_step=epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(onlineQNetwork.state_dict(), 'sql-policy.para')\n",
    "            print('Ep {}\\tMoving average score: {:.2f}\\t'.format(epoch, episode_reward))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
